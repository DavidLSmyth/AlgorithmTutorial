Algorithm.create!([
  {name: "Binary Search", description: "When we have a large collection of things, (e.g. names in a phone book), it is useful to be able to search for the existence of a specific value. One way of doing this is a brute force search which involves checking if first value in the collection matches the lookup value, then checking if the second value matches, until we find a match. If we have a lot of values in the collection, this can take a very long time. If the collection has inherent structure, which is often the case, then we can take advantage of this. A common occurrence of this is when a collection of values is sorted. In this case, we can make an intelligent 'guess' as to where the value might be. We can then try and improve this guess with a secondary guess and so on, until we find the lookup value or conclude that the lookup value doesn't exist in the list. This can yield far better results than the brute force method. This is the main motivation behind this algorithm.\r\n\r\nThe algorithm itself is as follows: Given an ordered array/list: \r\n1).Check if the list is empty. If so, the lookup value can't be found. Otherwise continue to step 2.\r\n2).Check if the lookup value is equal to the median value in the list. If so, return the median position in the original array as the found position. If not, continue to step 3.\r\n3).Check if the lookup value is greater than the median value in the list. If so, start again from step 1 with values from the median value onwards. Otherwise, start again from step 1 with values from the beginning of the array as far as the median value.\r\n\r\nPossibly add python gif here.", link: "https://blog.penjee.com/wp-content/uploads/2015/04/binary-and-linear-search-animations.gif", pseudoCode: "", complexityAnalysis: "Best Case Scenario: If the value is in the middle of the array, then we find the algorithm take O(1) seconds (0 iterations).\r\nWorst Case Scenario: If the value is at either extreme of the array (first or last position), then we need to perform log2(n) splits and one comparison is carried out at each comparison. So the algorithm takes at worst log2(n) seconds.\r\nAverage Case Scenario: We can assume that each value will be looked up with uniform probability. Then for an array of size n, we have 1 value that will be found on the zeroth iteration, two values that can be found on the first iteration, four values that can be found on the second iteration,....,2^n values that can be found on nth iteration (see diagram). Given that we perform one comparison at each iteration,then the average number of iterations needed to find a random value is sum(iteration*(number of values in iteration level/number of values in array)) = from 1 to n sum(i*(2^i-1)/n  (note: this was derived using expectation from probability)", difficulty: 3, author: "David", purpose: "Given a sorted collection , locate the position of  a specified element in the collection or verify that the element does not exist.", Python: "#python sample code (lots of variants of this algorithm - this one isn't consistent with complexity)\ndef binSearch(array,lookupValue,position,iterations):\n\titerations=iterations+1\n\tif len(array)==0:\n\t\treturn \"Value not found\"\n\telse:\n\t\tmidpoint=len(array)/2\n\t\tprint(array,midpoint)\n\t\tprint(position)\n\t\tprint('iterations',iterations)\n\t\tif array[midpoint]==lookupValue:\n\t\t\treturn \"Found at position: \"+str(int(arraySize*(position))+1)\n\t\telif lookupValue>array[midpoint]:\n\t\t\treturn binSearch(array[midpoint:],lookupValue,position+(0.5**iterations),iterations)\n\t\telse:\n\t\t\treturn binSearch(array[:midpoint],lookupValue,position-(0.5**iterations),iterations)\ndef binSearchInit(array,lookupValue):\n\tglobal arraySize\n\tarraySize=len(array)\n\treturn binSearch(array,lookupValue,0.5,1)", Java: "public static void main(String[] args){\n        int[] searchMe;\n        int someNumber;\n        ...\n        int index = binarySearch(searchMe, someNumber, 0, searchMe.length);\n        System.out.println(someNumber + ((index == -1) ? \" is not in the array\" : (\" is at index \" + index)));\n        ...\n}\n \npublic static int binarySearch(int[] nums, int check, int lo, int hi){\n        if(hi < lo){\n                return -1; //impossible index for \"not found\"\n        }\n        int guess = (hi + lo) / 2;\n        if(nums[guess] > check){\n                return binarySearch(nums, check, lo, guess - 1);\n        }else if(nums[guess]<check){\n                return binarySearch(nums, check, guess + 1, hi);\n        }\n        return guess;\n}", Cpp: "template <class T>\nint binsearch(const T array[], int len, T what)\n{\n  if (len == 0) return -1;\n  int mid = len / 2;\n  if (array[mid] == what) return mid;\n  if (array[mid] < what) {\n    int result = binsearch(array+mid+1, len-(mid+1), what);\n    if (result == -1) return -1;\n    else return result + mid+1;\n  }\n  if (array[mid] > what)\n    return binsearch(array, mid, what);\n}\n \n#include <iostream>\nint main()\n{\n  int array[] = {2, 3, 5, 6, 8};\n  int result1 = binsearch(array, sizeof(array)/sizeof(int), 4),\n      result2 = binsearch(array, sizeof(array)/sizeof(int), 8);\n  if (result1 == -1) std::cout << \"4 not found!\" << std::endl;\n  else std::cout << \"4 found at \" << result1 << std::endl;\n  if (result2 == -1) std::cout << \"8 not found!\" << std::endl;\n  else std::cout << \"8 found at \" << result2 << std::endl;\n \n  return 0;\n}", Ruby: nil},
  {name: "Merge Sort", description: "MergeSort is an algorithm that can efficiently sort a collection from highest to lowest (or visa versa). This depends on the elements on the list having a definition for order (e.g. 3>2 numeric, a>b alphabetic,\r\n etc.). It falls into the category of 'divide and conquer' algorithms. The motivation for the algorithm is that ordering two lists that are already ordered is easier than trying to order two unordered lists \r\n(try it yourself!). So if you break down the algorithm into a lot of small, sorted sub-lists, then it should be easy to rebuild the full list by recursively combining all of these sorted smaller sub-lists. \r\nA list of length 1 is ordered (trivially) so this is a good base case for our algorithm. Starting with elements of length one, we can combine these to make ordered lists of length two, then combine these to make\r\n ordered lists of length four, etc., until we recover the full ordered list. So we have a strategy that seems like it could work. The next step is deciding how to design the algorithm so that a computer can manage\r\n it efficiently. It's clear that we need to define a function that can take an unordered collection and return the same ordered collection. First we will recursively split the lists into lots of smaller sublists.\r\n We can do this using a splitting function that will split a list into two parts if the length of the list is greater than one, otherwise the list is trivially sorted and so can be returned as is. Picture: \r\n                \r\n                      List/Array Size                                 Stack Trace                 \r\n               \r\n                  **********************                  |     Start recursive algorithm here (level 0)\r\n                         split                            |\r\n                       /       \\                          |\r\n               ***********     ************               |          First recursion (level 1)\r\n                 split             split                  |                 .\r\n                /     \\           /     \\                 |                 .\r\n           ******   ******     *******   ******           |                 .\r\n           split    split       split     split           |                 .\r\n            /  \\     /  \\        /  \\      /  \\           |                 .                  \r\n             .         .          .         .             |                 .\r\n             .         .          .         .             |                 .\r\n             .         .          .         .             |                 .\r\n        /\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\        |                 .                                   \r\n        * * * * * * * * * * * * * * * * * * * * * *       |        Base case returned (level log2(n))\r\n\r\nWe can visualise the function that we have written as executing as shown in the diagram. The sublists of length one will be returned, so we have an algorithm that will recursively split a large sublist into lists\r\n of length one by recursively splitting the original list into two sublists. At the final level, where each list is of length one, we want to merge each of these trivially sorted sublists together and then merge\r\n the resulting sorted lists, and then merge these sorted sublists etc. until we recover the original list. It should be clear that for each of these levels, we want to call a merge operation on each pair of \r\nsublists. But this can easily be done! Since each split call results in two sublists, we can wrap each split call in a merge call (i.e. merge(split(leftList),split(RightList))). This ensures that as each of the \r\nsorted sublists are returned, they are again merged together and will return a combined sorted list. Thus we have ensured that as we work our way back up the tree, each pair of sublists is recombined to return a\r\n sorted list.\r\nThe stack should now look like this:\r\n                \r\n                      List/Array Size                                 Stack Trace                 \r\n               \r\n             Split(**********************)                |     Start recursive algorithm here (level 0)\r\n                       /       \\                          |\r\n        M(S(***********)      S(************))            |        First recursion (level 1)\r\n              /     \\                /     \\              |                 .\r\n    M(S(******)  S(******))  M(S(*******)   S(******))    |                 .\r\n             .         .          .         .             |                 .\r\n             .         .          .         .             |                 .\r\n             .         .          .         .             |                 .\r\n             .         .          .         .             |                 .\r\n       /\\    /\\     /\\      /\\     /\\     /\\    /\\        |                 .                                   \r\n    M(* *) M(* *) M(* *) M(* *) M(* *) M(* *) M(* *)      |      Base case returned (level log2(n)).\r\n\r\n(M stands for merge, S for split)\r\n\r\n\r\nThe stack trace is more difficult for this diagram. Since the Merge function is called within the Split function and the Split function is called within the Merge function (within the split function!), \r\nwe get a complex recursive function. You can imagine that each of the merge(split(LeftList),split(RightList)) as place holders. We can see that replacing each split function, we get: \r\nlist 1 = merge(split(LeftList),split(RightList))\r\nlist2 = merge(merge(split(LeftList),split(RightList)),merge(split(LeftList),split(RightList)))\r\n.\r\n.\r\nBut when the base case is returned, we get merge(...merge(merge(list of length 1,list of length 1),merge(list of length 1, list of length 1)),merge(merge(list of length 1,list of length 1),merge(list of length1, \r\nlist of length 1)),...\r\n\r\nSo then the recursive merge calls can execute and first merge the lists of length one to lists of length 2, which are then recursively merged into lists of length four, etc. until the first merge on the stack is \r\nexecuted and the original list (sorted) is returned. This take some thought to understand. The recursive Split function has the effect of stacking the Merge operations which are then executed once the Split \r\nfunction meets its base case (where all sublists are of length one). Adding print statements to the code are invaluable in understanding this.\r\n\r\nPerhaps think of it this way: \r\n               \r\n               S(****)\r\n               /       \\ \r\n        M(S(**)      S(**))\r\n       /     \\       /     \\ \r\n    M(M((*)  (*))  M((*)   (*)))\r\n         \\   /         \\  /                                      \r\n        M((**)         (**))\r\n              \\        /\r\n                (****)\r\n\r\nTry writing this out for a list of length 8 - this becomes a bit messy but the process is the exact same.", link: "https://upload.wikimedia.org/wikipedia/commons/c/cc/Merge-sort-example-300px.gif", pseudoCode: "", complexityAnalysis: "This algorithm will always perform it's split operation in exactly the same way; it will recursively break the collection in half until sublists of length one remain. Denote the length of the original collection by l0. Then the lengths of the lists will be l0, l0/2, l0/4, .... 1. How many of these splits occur? Let's construct a formula for the length of the sublists at split number n:\r\n\r\nlength of sublists at split number n = l(n)= l0 * (1/2^n)\r\nthen solve for n when the sublists are of length one:\r\n1 = l0 * (1/2^n)  =>  take log base 2 of both sides  =>  log2(1) = log2(l0*(2^-n))  =>  0 = log2(l0)+log2(2^-n)  =>  0 = log2(l0)-nlog2(2)  =>  log2(lo)=n\r\nso we will always have log2(lo) levels before the split function returns sublists of length 1, where at level n we will have 2^n splits (where level 0 corresponds to the full list). The divide step occurs in constant time; we aren't really doing any computation here, just computing the midpoint of the array. So asymptotically we can think of this operation taking O(1) time (big O notation hyperlink). The merge operations can vary, so this is where we will break the algorithm up into best case, worst case and average case.\r\n\r\n\r\nBest case scenario: If the collection is already sorted, then the mere operations will occur as follows: \r\nExample: Collection is [1,2,3,4,5]. Then after the split function has returned sublists of length one, the merge operation occurs as follows: \r\n\r\n(merge [1] & [2] and merge [3] & [4] and merge [5] & [])  1 < 2 => return [1,2] 3<4 => return [3,4]\r\n\r\n(merge [3,4] & [5]) 3<5 => [3]  4<5 => [3,4] => since left sublist used up, sorted list must be [3,4,5] => return [3,4,5]\r\n\r\n(merge [1,2,3] & [4,5])  1<3 => [1]  2<3 => [1,2] => since left sublist used up, sorted list must be [1,2,3,4,5] => return [1,2,3,4,5]\r\n\r\nabstracting this behaviour out, we see that there will be n/2 comparisons (equates to operations) for merging each sublist, therefore at any level n we will have l0/2 comparisons (think about this!). Then we have l0/2 operations at each level, with log2(lo) levels in total, so this gives us a running time of O(l0/2*log2(lo)) but asymptotically this is equivalent to O(lo*log2(l0)).\r\nIt's worth noting that this scenario also occurs when the list is 'reverse sorted' i.e. sorted in descending order.\r\n\r\nWorst case scenario: It should be clear that in the worst case this algorithm will have to perform a maximum amount of comparisons in the merge operation. So when does this happen? The recursive nature of the algorithm implies that a collection like [0,2,4,6,1,3,5,7] will require the maximum number of comparisons at the second last step. Then for [0,2,4,6] the worst case will occur when [0,4] and [2,6] are present in the second last merge and similarly for [1,3,5,7], [1,5] and [3,7]  are present in the second last merge.\r\n\r\nThen working backwards:\r\n\r\n                      [0,1,2,3,4,5,6,7]  \r\n                           /     \\\r\n                          /       \\\r\n                 [0,2,4,6]       [1,3,5,7] \r\n                     / \\           / \\\r\n                    /   \\         /   \\\r\n                 [0,4] [2,6]    [1,5] [3,7]\r\n                   |     |        |     |\r\n                   |     |        |     |\r\n                 [4,0] [6,2]    [5,1] [7,3]       \r\n                   \\     /        \\     /                        \r\n                    \\   /          \\   /\r\n                  [4,0,6,2]     [5,1,7,3]\r\n                      \\             /\r\n                       \\           / \r\n                     [4,0,6,2,5,1,7,3]\r\n\r\nSo it's clear from the example that n comparisons are carried out for each sublist of length n at every sublevel. Since at each sublevel the number of elements in each sublists combine to give l0, the number of elements in the original collection, then there must be l0 comparisons at each level. l0 comparisons at log2(l0) sublevels gives O(l0*log2(l0)) running time\r\n\r\nAverage case:\r\n\r\n\r\n                      \r\n\r\n\r\n\r\n", difficulty: 5, author: "David", purpose: "Sorts a collection of unordered elements", Python: "", Java: "import java.util.Arrays;\n\npublic class Merge implements SortingAlgorithms{\n\tString name=\"Merge\";\n\t@Override\n\tpublic int[] sort(int[] data) {\n\t\t// TODO Auto-generated method stub\n\t\t//while the list has more than one element, divide\n\t\tif(data.length>1){\n\t\t\t//set the midpoint\n\t\t\tint mp=data.length/2;\n\t\t\t//create a left list and a right list either side of the midpoint to be recursively split and then\n\t\t\t//conquered once they are of length one\n\t\t\tint[]ll= Arrays.copyOfRange(data, 0, mp);\n\t\t\tint[]rl= Arrays.copyOfRange(data, mp, data.length);\n\t\t\tsort(ll);\n\t\t\tsort(rl);\n\t\t\t//initialise the counter variables\n\t\t\tint i=0;\n\t\t\tint j=0;\n\t\t\tint k=0;\n\t\t\t//i holds the position for the right list, j for the left list\n\t\t\t//use a while loop to check that elements are still left in each list\n\t\t\t//k holds the position of the conquered array\n\t\t\twhile(i<ll.length && j<rl.length){\n\t\t\t\t//conquer array by adding the smallest element from the leftlist or rightlist until \n\t\t\t\t//one of the lists is empty, in which case add the rest of the elements of the non empty list\n\t\t\t\tif (ll[i]<=rl[j]){\n\t\t\t\t\tdata[k]=ll[i];\n\t\t\t\t\ti+=1;\n\t\t\t\t}\n\t\t\t\telse{\n\t\t\t\t\tdata[k]=rl[j];\n\t\t\t\t\tj+=1;\n\t\t\t\t}\n\t\t\t\tk+=1;\n\t\t\t}\n\t\t\t\n\t\t\twhile(i<ll.length){\n\t\t\t\tdata[k]=ll[i];\n\t\t\t\ti+=1;\n\t\t\t\tk+=1;\n\t\t\t}\n\t\t\twhile (j<rl.length){\n\t\t\t\tdata[k]=rl[j];\n\t\t\t\tj+=1;\n\t\t\t\tk+=1;\n\t\t\t\t\n\t\t\t}\n\t\t}\n\t\treturn data;\n\t}\n\n\t@Override\n\tpublic String name() {\n\t\t// TODO Auto-generated method stub\n\t\treturn name;\n\t}\n\n\t@Override\n\tpublic void name(String name) {\n\t\t// TODO Auto-generated method stub\n\t\tthis.name=name;\n\t}\n}", Cpp: nil, Ruby: nil},
  {name: "Quick Sort", description: "Similar to merge sort. Quick sort works by taking an element and then putting all elements less than it in a 'left' sublist, and all elements greater than it in a 'right' sublist. This breaks the original list down recursively until each list is of length one, when the recursion rebuilds these lists to a sorted list. There", link: "http://interactivepython.org/runestone/static/pythonds/_images/partitionA.png", pseudoCode: "public class Quick implements SortingAlgorithms {\r\n\tprivate String name=\"Quick\";\r\n\t//@Override\r\n\tprivate void quicksort(int data[], int lo, int hi) {\r\n        //data is sorted when the lo value overtakes the hi value\r\n        if (lo >= hi) {\r\n            return;\r\n        }\r\n        int pivot = data[lo];\r\n        int i = lo - 1;\r\n        int wall = hi + 1;\r\n        //divide the list into two sublist, one with values less than the pivot\r\n        //and one with values greater than the pivot\r\n        while (i < wall) {\r\n            // Keep increasing if the values are less than the pivot\r\n            i++;\r\n            while (data[i] < pivot){\r\n            \ti++; \r\n            }\r\n            // Keep decreasing if the values are greater than the pivot\r\n            wall--;\r\n            while (data[wall] > pivot) { \r\n            \twall--; \r\n            }\r\n            // swap the pivot with the value that was out of place\r\n            if (i < wall) {\r\n            \tint temp = data[i];\r\n                data[i] = data[wall];\r\n                data[wall] = temp;\r\n            }\r\n        }\r\n        \r\n        // Use recursion to sort the values less than the pivot and greater than the pivot\r\n        quicksort(data, lo, wall);\r\n        quicksort(data, wall + 1, hi);\r\n    }\r\n\tpublic int[] sort(int[] data) {\r\n\t\tquicksort(data,0,data.length-1);\r\n\t\treturn data;\r\n\t}\r\n\t@Override\r\n\tpublic String name() {\r\n\t\t// TODO Auto-generated method stub\r\n\t\treturn name;\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void name(String name) {\r\n\t\tthis.name=name;\r\n\t\t\r\n\t}\r\n}", complexityAnalysis: "daf", difficulty: 2, author: "David", purpose: "Sorts a list of underordered elements", Python: nil, Java: nil, Cpp: nil, Ruby: nil},
  {name: "New algo", description: "lj", link: "lj.png", pseudoCode: "ln", complexityAnalysis: "ln", difficulty: 1, author: "David", purpose: "jl", Python: nil, Java: nil, Cpp: nil, Ruby: nil},
  {name: "Gradient Descent", description: "Here is the description", link: "abcd.gif", pseudoCode: "#will be here eventually", complexityAnalysis: "coming soon", difficulty: 3, author: "David", purpose: "Given a convex function, the gradient descent algorithm will find the values of the function parameters that minimise the function", Python: nil, Java: nil, Cpp: nil, Ruby: nil},
  {name: "Linear Search", description: "In linear search, we look at each item in the list in turn,\r\nquitting once we find an item that matches the search term or once we’ve reached the end of the list. Our\r\n“return value” is the index at which the search term was found, or some indicator that the search term was\r\nnot found in the list.", link: "smileyFace.gif", pseudoCode: "function linearSearch (collection,element)\r\n   int index=-1\r\n   for int i in 1->length of collection do\r\n      if(collection[i] equals element){\r\n         index=i\r\n         break loop\r\n   if(index==-1){\r\n      return string(element not found)\r\n   }\r\n   else{\r\n      return string(element found at position)+string(index)\r\n\r\n   \r\n      \r\n", complexityAnalysis: "Best case: Element located in position 1 i.e. the algorithm only needs to make one comparison. This takes constant time to find (and is trivial) so time taken is O(1)\r\n\r\nAverage case: Assuming that element is picked at random. Let n denote the length of the list. The number of comparisons needed to find the element give an approximation of the amount of time needed for the algorithm to complete, therefore the run time of the algorithm in the average case corresponds to the expected position of a random element in the list. The expected position to find this element is given by SUM(element position * 1/n where 1/n gives the probability of finding a random element in the list at position i. This gives 1/n * SUM(element position) where the sum is over all element positions i.e. SUM(1,2,...,n)=n(n+1)/2. So we have (1/n)*(n*(n+1)/2)=(n+1)/2=O(n)\r\n\r\nWorst case: It's obvious that this algorithm will perform at its worst when the lookup element is at the last position in the list. In this case, the algorithm will have to perform a maximal number of comparisons, one for each element in the collection. So the algorithm needs to perform n comparisons, which gives a runtime of O(n) seconds.", difficulty: 2, author: "David", purpose: "Given a collection, find the location of an element or verify that it doesn't exist", Python: "def sequentialSearch(alist, item):\n\t    pos = 0\n\t    found = False\n\t\n\t    while pos < len(alist) and not found:\n\t        if alist[pos] == item:\n\t            found = True\n\t        else:\n\t            pos = pos+1\t\n\t    return found", Java: "public int sequentialSearch(int item, int[] list) {\n\t// if index is still -1 at the end of this method, the item is not\n\t// in this array.\n\tint index = -1;\n\t// loop through each element in the array. if we find our search\n\t// term, exit the loop.\n\tfor (int i=0; i<list.length; i++) {\n\t\tif (list[i] == item) {\n\t\t\tindex = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn index;\n}", Cpp: nil, Ruby: nil},
  {name: "Bogo Sort", description: "The algorithm successively generates permutations of its input until it finds one that is sorted.", link: "https://media.giphy.com/media/YIgKKIj5d4CQ/giphy.gif", pseudoCode: "while not isInOrder(collection):\r\n    shuffle(collection)", complexityAnalysis: "BogoSort takes ages", difficulty: 1, author: "David", purpose: "Sorts a collection from highest to lowest", Python: nil, Java: nil, Cpp: nil, Ruby: nil}
])
ActsAsTaggableOn::Tagging.create!([
  {tag_id: 1, taggable_id: 2, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 2, taggable_id: 2, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 2, taggable_id: 1, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 3, taggable_id: 9, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 4, taggable_id: 9, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 5, taggable_id: 9, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 1, taggable_id: 3, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 6, taggable_id: 1, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 6, taggable_id: 10, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 2, taggable_id: 10, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 7, taggable_id: 11, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 1, taggable_id: 11, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"}
])
ActsAsTaggableOn::Tag.create!([
  {name: "Sorting", taggings_count: 3, description: "Sorting algorithms take a collection of elements and return the same collection which has been ordered. "},
  {name: "Donald Knuth", taggings_count: 3, description: "Donald Ervin Knuth (born January 10, 1938) is an American computer scientist, mathematician, and professor emeritus at Stanford University.\r\n\r\nHe is the author of the multi-volume work The Art of Computer Programming. He contributed to the development of the rigorous analysis of the computational complexity of algorithms and systematized formal mathematical techniques for it. In the process he also popularized the asymptotic notation. In addition to fundamental contributions in several branches of theoretical computer science, Knuth is the creator of the TeX computer typesetting system, the related METAFONT font definition language and rendering system, and the Computer Modern family of typefaces.\r\n\r\nAs a writer and scholar, Knuth created the WEB and CWEB computer programming systems designed to encourage and facilitate literate programming, and designed the MIX/MMIX instruction set architectures. Knuth strongly opposes granting software patents, having expressed his opinion to the United States Patent and Trademark Office and European Patent Organization."},
  {name: "Optimisation", taggings_count: 1, description: nil},
  {name: "Cauchy", taggings_count: 1, description: nil},
  {name: "Hadamard", taggings_count: 1, description: nil},
  {name: "Search", taggings_count: 2, description: "Search algorithms come in a few different flavours: greedy, all paths, etc. They either deal with an ordered collection or an unordered collection. A search algorithms will attempt to traverse a search space in a systematic manner in order to locate the existence/position of an element."},
  {name: "Eric Moore", taggings_count: 1, description: "Eric Moore is currently a student in NUIG and is credited with the invention of the BogoSort sorting algorithm"}
])
Tagging.create!([
  {tag_id: 1, taggable_id: 2, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 2, taggable_id: 2, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 2, taggable_id: 1, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 3, taggable_id: 9, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 4, taggable_id: 9, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 5, taggable_id: 9, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 1, taggable_id: 3, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 6, taggable_id: 1, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 6, taggable_id: 10, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 2, taggable_id: 10, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 7, taggable_id: 11, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"},
  {tag_id: 1, taggable_id: 11, taggable_type: "Algorithm", tagger_id: nil, tagger_type: nil, context: "tags"}
])
